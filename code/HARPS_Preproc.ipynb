{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for preprocessing the solar observations to make them ready for input to the autoencoder network.\n",
    "\n",
    "The notebook performs the following preprocessing steps. \n",
    " 1. Load Data\n",
    " 2. Connect observations with correct blaze and wave files\n",
    " 3. Blaze correct spectra\n",
    " 4. Filter out low flux and very high airmass observations \n",
    " 5. Interpolate all observations to common wavelength grid \n",
    " 6. Take natural logarithm and continuum normalise all observations\n",
    "\n",
    "The notebook is set up to perform preprocessing of observations in the format of newer HARPS-N 2D solar observations. If one whises to train on solar spectra from another spectrograph this notebook will not work. You can either modify this notebook to the data format of the spectrograph or carry out your own preprocessing. The important part is to obtain blaze corrected continuum normalised log spectra interpolated to the same wavelength axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from astropy.io import fits\n",
    "import h5py\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import math\n",
    "from spectrum_overload import Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data was loaded in 46.950289726257324 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# Initializing \n",
    "data_matrix=[]\n",
    "blaze_matrix=[]\n",
    "wave_matrix=[]\n",
    "airmass = []\n",
    "berv = []\n",
    "file_name = []\n",
    "wave_name = []\n",
    "blaze_name = []\n",
    "wave_to_use = []\n",
    "blaze_to_use = []\n",
    "\n",
    "for path, subdirs, files in os.walk(\"/projects/data/HARPS/solar/2020/\"):\n",
    "    for name in files:       \n",
    "        # Loading Blaze files\n",
    "        if name.endswith(\"blaze_A.fits\"):\n",
    "            fn = os.path.join(path, name) #filename\n",
    "            blaze = fits.getdata(fn)\n",
    "            blaze_matrix.append(blaze)\n",
    "            blaze_name.append(fn)            \n",
    "\n",
    "        # Loading Wave files  \n",
    "        if name.endswith(\"wave_A.fits\"):\n",
    "            fn = os.path.join(path, name) #filename\n",
    "            wave = fits.getdata(fn)            \n",
    "            wave_matrix.append(wave)\n",
    "            wave_name.append(fn)\n",
    "            \n",
    "        # Loading Spectrum files\n",
    "        if name.endswith(\"e2ds_A.fits\"):\n",
    "            fn       = os.path.join(path, name) #filename\n",
    "            tmp_data = fits.getdata(fn)       #read data entry \n",
    "            header = fits.getheader(fn)      # read header\n",
    "            \n",
    "            # Select only solar observations\n",
    "            data_matrix.append(tmp_data)\n",
    "            airmass.append(max(header['HIERARCH ESO TEL AIRM START'], header['HIERARCH ESO TEL AIRM END']))\n",
    "            berv.append(header['HIERARCH ESO DRS BERV'])\n",
    "            wave_to_use.append(header['HIERARCH ESO DRS CAL TH FILE'])\n",
    "            blaze_to_use.append(header['HIERARCH ESO DRS BLAZE FILE'])\n",
    "            file_name.append(fn)\n",
    "\n",
    "print(\"Data was loaded in\", time.time()-start, \"seconds\")\n",
    "spectrum = np.asarray(data_matrix)\n",
    "Airmass = np.asarray(airmass)\n",
    "Berv = np.asarray(berv)\n",
    "blaze = np.asarray(blaze_matrix)\n",
    "wave = np.asarray(wave_matrix)\n",
    "blaze_name = np.asarray(blaze_name)\n",
    "wave_name = np.asarray(wave_name)\n",
    "blaze_to_use = np.asarray(blaze_to_use)\n",
    "wave_to_use = np.asarray(wave_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spectrum.shape=(883, 72, 4096)\n",
      "Airmass.shape=(883,)\n",
      "Berv.shape=(883,)\n",
      "blaze.shape=(883, 72, 4096)\n",
      "wave.shape=(883, 72, 4096)\n",
      "blaze_name.shape=(883,)\n",
      "wave_name.shape=(883,)\n",
      "blaze_to_use.shape=(883,)\n",
      "wave_to_use.shape=(883,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{spectrum.shape=}\\n{Airmass.shape=}\\n{Berv.shape=}\\n{blaze.shape=}\\n{wave.shape=}\\n{blaze_name.shape=}\\n{wave_name.shape=}\\n{blaze_to_use.shape=}\\n{wave_to_use.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting spectra to their respective wave and blaze files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HARPS -> 72 orders, from 378-691 nm (89-161)\n",
    "# HARPS-N -> 69 orders, from 385-691 nm (92-161) (?)\n",
    "# TAU is trained on HARPS-N solar observation, so we need to cut the HARPS orders to match the HARPS-N orders\n",
    "\n",
    "spectrum, wave, blaze = spectrum[:, 3:, :], wave[:, 3:, :], blaze[:, 3:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blaze correcting spectra\n",
    "corrected_spectrum = spectrum / blaze\n",
    "\n",
    "#Combining blaze corrected flux and wavelength into data array\n",
    "Data = np.array([corrected_spectrum, wave])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing blaze files\n",
      "No missing wave files\n"
     ]
    }
   ],
   "source": [
    "# Checking if all wave and blaze files were found\n",
    "missing = []\n",
    "for i in range(len(blaze)):\n",
    "    if [i,0,0]==0:\n",
    "        missing.append(i)\n",
    "        \n",
    "missing = np.array([]).astype(int)\n",
    "missing = np.asarray(missing)  \n",
    "if len(missing)==0:\n",
    "    print('No missing blaze files')\n",
    "if len(missing)>0:\n",
    "    print(len(missing),'Missing blaze file')\n",
    "    print(blaze_to_use[missing])\n",
    "    print('For observation')\n",
    "    print(missing)\n",
    "\n",
    "missing = []\n",
    "for i in range(len(correct_wave)):\n",
    "    if correct_wave[i,0,0]==0:\n",
    "        missing.append(i)\n",
    "        \n",
    "missing = np.array([]).astype(int)\n",
    "missing = np.asarray(missing)   \n",
    "if len(missing)==0:\n",
    "    print('No missing wave files')\n",
    "if len(missing)>0:\n",
    "    print(len(missing),'Missing wave files')\n",
    "    print(wave_to_use[missing])\n",
    "    print('For observation')\n",
    "    print(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering away observations with missing wave / blaze files:\n",
    "if len(missing)>0:\n",
    "    Data = np.delete(Data, missing ,axis=1)\n",
    "    Airmass = np.delete(Airmass,missing,axis=0)\n",
    "    Berv = np.delete(Berv,missing,axis=0)\n",
    "    print('Number of removed spectra for missing blaze or wave files',len(missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed spectra for high airmass: 268\n"
     ]
    }
   ],
   "source": [
    "# Filtering away observation with airmass > 2.0\n",
    "index1 = []\n",
    "for i in range(len(Airmass)):\n",
    "    if  Airmass[i]  > 2: \n",
    "        index1.append(i)   \n",
    "\n",
    "# Filtering the observations from index out \n",
    "if len(index1)>0:\n",
    "    Data = np.delete(Data, index1 ,axis=1)\n",
    "    Airmass = np.delete(Airmass,index1,axis=0)\n",
    "    Berv = np.delete(Berv,index1,axis=0)\n",
    "    file_name = np.delete(file_name,index1,axis=0)\n",
    "    \n",
    "    print('Number of removed spectra for high airmass:',len(index1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed spectra for low flux: 611\n"
     ]
    }
   ],
   "source": [
    "# Finding mean flux of spectra\n",
    "means = np.zeros(len(Data[0]))\n",
    "for i in range(len(Data[0])):\n",
    "    means[i] = np.mean(Data[0,i,:])\n",
    "    \n",
    "#Some of the observations have very low flux values. \n",
    "# Finding index of low flux observations\n",
    "index2 = []\n",
    "for i in range(len(Data[0])):\n",
    "    if  np.max(means) / np.mean(Data[0,i,:])  > 1.2: \n",
    "        index2.append(i)       \n",
    "\n",
    "# Filtering the low flux observations from index out \n",
    "if len(index2)>0:\n",
    "    filtered_Data = np.delete(Data, index2 ,axis=1)\n",
    "    Airmass = np.delete(Airmass,index2,axis=0)\n",
    "    Berv = np.delete(Berv,index2,axis=0)\n",
    "    file_name = np.delete(file_name,index2,axis=0)\n",
    "    \n",
    "    print('Number of removed spectra for low flux:',len(index2)) \n",
    "else:\n",
    "    filtered_Data = Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['../solar/ADP.2018-11-10T01:08:51.333/HARPS.2018-11-08T19:28:04.297_e2ds_A.fits',\n",
       "       '../solar/ADP.2018-09-15T01:19:14.928/HARPS.2018-09-13T18:30:05.963_e2ds_A.fits',\n",
       "       '../solar/ADP.2018-11-10T01:08:51.355/HARPS.2018-11-08T19:52:41.978_e2ds_A.fits',\n",
       "       '../solar/ADP.2018-11-10T01:08:51.395/HARPS.2018-11-08T20:16:22.036_e2ds_A.fits'],\n",
       "      dtype='<U78')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = filtered_Data.transpose(1,2,0,3) # Transposing data array to have observations as first index\n",
    "\n",
    "# For the specific training data we are using \n",
    "# orders 0,1,2,6 and 25 have negative flux values in their spectrum. \n",
    "# These are converted to a flux value of 1 for stability when taking the natural logarithm, \n",
    "D[:,0,0,:] = np.where(D[:,0,0,:]<1, 1, D[:,0,0,:])\n",
    "D[:,1,0,:] = np.where(D[:,1,0,:]<1, 1, D[:,1,0,:])\n",
    "D[:,2,0,:] = np.where(D[:,2,0,:]<1, 1, D[:,2,0,:])\n",
    "D[:,6,0,:] = np.where(D[:,6,0,:]<1, 1, D[:,6,0,:])\n",
    "D[:,25,0,:] = np.where(D[:,25,0,:]<1, 1, D[:,25,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape\n",
      "(4, 69, 2, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Finding mean flux of spectra and saving for later scaling \n",
    "means = np.zeros(len(D))\n",
    "for i in range(len(D)):\n",
    "    means[i] = np.mean(D[i,:])\n",
    "const = np.log(np.mean(means))\n",
    "\n",
    "print('Data shape')\n",
    "print(D.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolating to common wavelength axis and continuum normalizing spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "K = D.shape[1]       # number of apertures to combine (here 69)\n",
    "resolution = D.shape[3] # number of pixels (here 4096)\n",
    "\n",
    "combined_wave = np.zeros((K,resolution))\n",
    "combined_flux = np.zeros((K,D.shape[0],resolution))\n",
    "for k in range(K):\n",
    "\n",
    "    # Interpolating to common restframe grid of wavepoints \n",
    "    aperture = k\n",
    "\n",
    "    # Finding min and max for interpolation. \n",
    "    # This is needed as the observations do not cover the exact same wavelength regions\n",
    "    MIN =  math.ceil(np.min(D[:,aperture,1,0]))   # Wave min\n",
    "    MAX =  math.floor(np.max(D[:,aperture,1,-1])) # Wave max\n",
    "\n",
    "    common_wave = np.linspace(MIN, MAX, num=resolution) \n",
    "    interpol_flux = []\n",
    "\n",
    "    for i in range(len(D)):\n",
    "        flux = D[i,aperture,0,:]  \n",
    "        wave = D[i,aperture,1,:]\n",
    "        f = interpolate.interp1d(wave, flux)\n",
    "\n",
    "        int_flux = f(common_wave)   # use interpolation function returned by `interp1d`\n",
    "        interpol_flux.append(int_flux)\n",
    "    interpol_flux = np.array(interpol_flux)\n",
    "    val_save = interpol_flux   \n",
    "    \n",
    "    # Continuum normalisation\n",
    "    # Can be performed with your own choice of continuum normalization procedure\n",
    "    # Here using spectrum_overload package\n",
    "    interpol_flux=np.log(interpol_flux) # taking log of spectrum \n",
    "    normalized_flux = np.zeros((len(interpol_flux),len(interpol_flux[0])))\n",
    "    for j in range(len(interpol_flux)):\n",
    "        s = Spectrum(flux=interpol_flux[j], xaxis=common_wave)\n",
    "        continuum = s.continuum(method=\"linear\", nbins=10, ntop=5) # Optimal continuum normalisaton params can depend on spectrum size\n",
    "        normalized_flux[j] = interpol_flux[j]/continuum.flux\n",
    "    interpol_flux=normalized_flux\n",
    "    \n",
    "    # Combining the wave function and flux of each aperture\n",
    "    combined_wave[k] = common_wave\n",
    "    combined_flux[k] = interpol_flux\n",
    "    \n",
    "Preproc_wave=combined_wave\n",
    "Preproc_flux=combined_flux.transpose(1,0,2) # (orders, observations, pixels) -> (observations, orders, pixels)\n",
    "Preproc_airmass = Airmass\n",
    "Preproc_berv = Berv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Preproc Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(\"../solar/preproc\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "with open('../solar/preproc/preproc_wave.pkl', 'wb') as pout:\n",
    "    pickle.dump(Preproc_wave, pout)\n",
    "\n",
    "output = open('../solar/preproc/preproc_flux.pkl', 'wb')\n",
    "pickle.dump(Preproc_flux, output)\n",
    "output.close()\n",
    "\n",
    "output = open('../solar/preproc/preproc_airmass.pkl', 'wb')\n",
    "pickle.dump(Preproc_airmass, output)\n",
    "output.close()\n",
    "\n",
    "output = open('../solar/preproc/preproc_berv.pkl', 'wb')\n",
    "pickle.dump(Preproc_berv, output)\n",
    "output.close()\n",
    "\n",
    "output = open('../solar/preproc/preproc_const.pkl', 'wb')\n",
    "pickle.dump(const, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file\n",
    "#output = open('../preproc/preproc_wave.pkl', 'wb')\n",
    "#pickle.dump(preproc_wave, output)\n",
    "#output.close()\n",
    "\n",
    "#output = open('../preproc/preproc_flux.pkl', 'wb')\n",
    "#pickle.dump(Preproc_flux, output)\n",
    "#output.close()\n",
    "\n",
    "#output = open('../preproc/preproc_airmass.pkl', 'wb')\n",
    "#pickle.dump(preproc_airmass, output)\n",
    "#output.close()\n",
    "\n",
    "#output = open('../preproc/preproc_berv.pkl', 'wb')\n",
    "#pickle.dump(preproc_berv, output)\n",
    "#output.close()\n",
    "\n",
    "#output = open('../preproc/preproc_const.pkl', 'wb')\n",
    "#pickle.dump(const, output)\n",
    "#output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
